{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7d898-350f-4c9a-baa0-fd831964432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import variable\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import datetime as dt\n",
    "from os.path import exists\n",
    "import shutil\n",
    "import pickle\n",
    "from PIL import ImageGrab, Image\n",
    "\n",
    "def mat_mul(A, B):\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "def count_parameters(model):\n",
    "     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class textcolors:\n",
    "    RED = \"\\033[31m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    YELLOW = \"\\033[33m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    DEFAULT = \"\\033[37m\"\n",
    "    ENDC = '\\033[0m'\n",
    "    \n",
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "\n",
    "def loss_fn(preds, label, mu, logvar):\n",
    "    # MSE = F.mse_loss(preds, label, reduction='sum')\n",
    "    MSE = F.smooth_l1_loss(preds, label, reduction='sum')\n",
    "    kld_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + kld_loss, MSE, kld_loss\n",
    "\n",
    "class PointFusion(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(PointFusion,self).__init__()\n",
    "\n",
    "        h_dim = 12\n",
    "        z_dim = 3\n",
    "        self.float()\n",
    "        \n",
    "        self.point_transform_net = nn.Sequential(\n",
    "            nn.Conv1d(3, 64, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 128, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(128, 1024, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2048),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, 256, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 9, bias=True)\n",
    "        ).to(device)\n",
    "\n",
    "        self.point_transform_forward = nn.Sequential(\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(3, 64, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 64, 1, bias=True),\n",
    "            nn.PReLU()\n",
    "        ).to(device)\n",
    "\n",
    "        self.feature_transform_net = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 128, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(128, 1024, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2048),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, 256, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 64*64, bias=True)\n",
    "        ).to(device)\n",
    "\n",
    "        self.feature_transform_forward = nn.Sequential(\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 64, 1, bias= True),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 128, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(128, 1024, 1, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2048),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "        self.fused_encoder = nn.Sequential(\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(3072, 1536, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(1536, 768, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(768, 384, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(384, 192, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(192, 96, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(96, 48, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(48, 24, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(24, h_dim, bias=True),\n",
    "            nn.PReLU()\n",
    "        ).to(device)\n",
    "\n",
    "        self.lat_mu = nn.Linear(h_dim,z_dim).to(device)\n",
    "        self.lat_logvar = nn.Linear(h_dim,z_dim).to(device)\n",
    "        self.sample = nn.Linear(z_dim,h_dim).to(device)\n",
    "\n",
    "        self.box_decoder = nn.Sequential(\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(h_dim + 4, 32, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(32, 64, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(64, 128, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(128, 256, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 512, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, 1024, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(1024, 1024, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(1024, 2048, bias=True),\n",
    "            nn.PReLU(),\n",
    "        ).to(device)\n",
    "\n",
    "        self.box = nn.Linear(2048, 24, bias=True).to(device)\n",
    "\n",
    "        init_modules = [self.point_transform_net, self.point_transform_forward, self.feature_transform_net, self.feature_transform_forward,\n",
    "                        self.lat_mu, self.lat_logvar, self.sample,self.fused_encoder,\n",
    "                        self.box_decoder,self.box]\n",
    "        \n",
    "        for module in init_modules:\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_().to(device)\n",
    "        esp = torch.randn(*mu.size()).to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "\n",
    "    def forward(self, x_point, x_image, x_2dbox):\n",
    "        a = self.point_transform_net(x_point)\n",
    "        input_T = a.view(x_point.shape[0],3,3)\n",
    "        g = mat_mul(x_point.transpose(1,2),input_T)\n",
    "        g = self.point_transform_forward(g.transpose(2,1))\n",
    "\n",
    "        b = self.feature_transform_net(g)\n",
    "        feature_T = b.view(x_point.shape[0],64,64)\n",
    "        g = mat_mul(g.transpose(1,2),feature_T)\n",
    "        lidar_feature = self.feature_transform_forward(g.transpose(2,1))\n",
    "\n",
    "        # print(x_image.shape, lidar_feature.shape)\n",
    "        fused_feature = torch.hstack((x_image,lidar_feature))\n",
    "        fused_encoded = self.fused_encoder(fused_feature)\n",
    "\n",
    "        fused_mu = self.lat_mu(fused_encoded)\n",
    "        fused_logvar = self.lat_logvar(fused_encoded)\n",
    "\n",
    "        fused_z = self.reparameterize(fused_mu,fused_logvar)\n",
    "        \n",
    "        fused_sample = self.sample(fused_z)\n",
    "        box_sample = torch.hstack((fused_sample,x_2dbox))\n",
    "\n",
    "        box_preds = self.box_decoder(box_sample)\n",
    "\n",
    "        boxes = self.box(box_preds)\n",
    "        #print(\"Model->boxes-\", boxes.shape)\n",
    "        #print(boxes)\n",
    "\n",
    "        return fused_mu, fused_logvar, boxes\n",
    "\n",
    "class textcolors:\n",
    "    RED = \"\\033[31m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    YELLOW = \"\\033[33m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    DEFAULT = \"\\033[m\"\n",
    "\n",
    "set_seeds(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "points =  np.load('train_points_pt.npy')\n",
    "labels = np.load('train_labels_pt.npy')\n",
    "boxes2d = np.load('train_boxes2d_pt.npy')\n",
    "intermediate_output = np.load('intermediate_output_pt.npy')\n",
    "# print(\"potints-\", points.shape)\n",
    "# print(\"labels-\", labels.shape)\n",
    "# print(\"boxes2d-\", boxes2d.shape)\n",
    "# print(\"intermediate-output-\", intermediate_output.shape)\n",
    "\n",
    "\n",
    "\n",
    "# print the model summary\n",
    "model = PointFusion(device=device)\n",
    "print(\"Model Summary (Trainable Parameters = {}):\\n\".format(count_parameters(model)))\n",
    "#print(model)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "optimizer_to(optimizer,device)\n",
    "\n",
    "# decide on train/val ratio\n",
    "train_points = points#[:x]\n",
    "test_points = points#[x:]\n",
    "\n",
    "train_boxes2d = boxes2d\n",
    "test_boxes2d = boxes2d\n",
    "\n",
    "train_labels = labels\n",
    "test_labels = labels\n",
    "\n",
    "train_intermediate = intermediate_output\n",
    "test_intermediate = intermediate_output\n",
    "\n",
    "#epoch number\n",
    "epo = 100\n",
    "\n",
    "train_bs = 16\n",
    "val_bs = 4\n",
    "\n",
    "perm1 = np.random.permutation(len(train_points))\n",
    "# print(\"perm1-\",perm1)\n",
    "train_points = train_points[perm1]\n",
    "train_labels = train_labels[perm1]\n",
    "train_boxes2d = train_boxes2d[perm1]\n",
    "train_intermediate = train_intermediate[perm1]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "point_train_batches = DataLoader(train_points, batch_size=train_bs)\n",
    "image_train_batches = DataLoader(train_intermediate,batch_size=train_bs)\n",
    "label_train_batches = DataLoader(train_labels, batch_size=train_bs)\n",
    "boxes_train_batches = DataLoader(train_boxes2d,batch_size=train_bs)\n",
    "#classes_train_batches = DataLoader(train_classes, batch_size=train_bs)\n",
    "\n",
    "point_val_batches = DataLoader(test_points, batch_size=val_bs)\n",
    "image_val_batches = DataLoader(test_intermediate,batch_size=val_bs)\n",
    "label_val_batches = DataLoader(test_labels, batch_size=val_bs)\n",
    "boxes_val_batches = DataLoader(test_boxes2d,batch_size=val_bs)\n",
    "#classes_val_batches = DataLoader(dev_classes, batch_size=val_bs)\n",
    "\n",
    "min_validation_loss = np.inf\n",
    "count = 0\n",
    "num_epoch = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "wait = 50\n",
    "\n",
    "for epoch in range(1, epo + 1):\n",
    "    print(\"\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    avg_train_loss = 0.0\n",
    "    avg_val_loss = 0.0\n",
    "    avg_train_box_mse = 0.0\n",
    "    avg_train_box_kld = 0.0\n",
    "    avg_val_box_mse = 0.0\n",
    "    avg_val_box_kld = 0.0\n",
    "    #avg_class_train_loss = 0.0\n",
    "    #avg_class_val_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    data = zip(point_train_batches,image_train_batches,label_train_batches,boxes_train_batches)\n",
    "    progressbar = tqdm(data, unit = ' Batch', total=len(point_train_batches))\n",
    "    for step, (x_points,x_images,y_boxes3d,y_boxes2d) in enumerate(progressbar):\n",
    "        x_images = x_images.to(device)\n",
    "        x_points = x_points.to(device)\n",
    "        y_boxes3d = y_boxes3d.to(device)\n",
    "        y_boxes2d = y_boxes2d.to(device)\n",
    "                \n",
    "        mu, logvar, boxes = model(x_points,x_images,y_boxes2d)\n",
    "\n",
    "        # print(\"shape-\", boxes.shape, y_boxes3d.shape)\n",
    "\n",
    "        # Properly reshaping\n",
    "        tmp = torch.empty(size=(boxes.shape[0], 8, 3)).to(device) \n",
    "        for i in range(boxes.shape[0]):\n",
    "            t2 = boxes[i]\n",
    "            t2_2d = torch.reshape(t2, shape=(8, 3))\n",
    "            # final = torch.cat([final, t2_2d], dim=0)\n",
    "            tmp[i] = t2_2d\n",
    "        # print(\"shape-\", tmp.shape, tmp.device, device)\n",
    "\n",
    "        \n",
    "        total_loss,box_mse,box_kld = loss_fn(tmp, y_boxes3d, mu, logvar)\n",
    "\n",
    "        # print(y_boxes3d.dtype)\n",
    "\n",
    "        \n",
    "        total_loss = total_loss.float()\n",
    "        box_mse = box_mse.float()  \n",
    "        # print(total_loss.dtype, box_mse.dtype, box_kld.dtype)\n",
    "        # print(total_loss)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_train_loss += total_loss.item()\n",
    "        avg_train_box_mse += box_mse\n",
    "        avg_train_box_kld += box_kld\n",
    "\n",
    "        progressbar.set_description(textcolors.DEFAULT + \"Epoch [{}/{}] | Training Batch Loss = {:.4f}\".format(epoch,epo,total_loss.item()))\n",
    "    \n",
    "    model.eval()\n",
    "    data = zip(point_val_batches,image_val_batches,label_val_batches,boxes_val_batches)\n",
    "    progressbar = tqdm(data, unit = ' Batch', total=len(point_val_batches))\n",
    "    for step, (x_points,x_images,y_boxes3d,y_boxes2d) in enumerate(progressbar):\n",
    "        \n",
    "        x_images = x_images.to(device)\n",
    "        x_points = x_points.to(device)\n",
    "        y_boxes3d = y_boxes3d.to(device)\n",
    "        y_boxes2d = y_boxes2d.to(device)\n",
    "\n",
    "        # print(f\"{textcolors.RED}line 361 - {x_points.shape}-{x_images.shape}-{y_boxes2d.shape}{textcolors.DEFAULT}\")\n",
    "     \n",
    "        with torch.no_grad():   \n",
    "            mu, logvar, boxes = model(x_points,x_images,y_boxes2d)\n",
    "\n",
    "        tmp = torch.empty(size=(boxes.shape[0], 8, 3)).to(device) \n",
    "        for i in range(boxes.shape[0]):\n",
    "            t2 = boxes[i]\n",
    "            t2_2d = torch.reshape(t2, shape=(8, 3))\n",
    "            # final = torch.cat([final, t2_2d], dim=0)\n",
    "            tmp[i] = t2_2d\n",
    "        total_loss,box_mse,box_kld = loss_fn(tmp, y_boxes3d, mu, logvar)\n",
    "\n",
    "        avg_val_loss += total_loss.item()\n",
    "        avg_val_box_mse += box_mse\n",
    "        avg_val_box_kld += box_kld\n",
    "\n",
    "        progressbar.set_description(textcolors.DEFAULT + \"Epoch [{}/{}] | Validation Batch Loss = {:.4f}\".format(epoch,epo,total_loss.item()))\n",
    "\n",
    "    avg_train_loss = avg_train_loss / len(point_train_batches)\n",
    "    avg_train_box_mse = avg_train_box_mse / len(point_train_batches)\n",
    "    avg_train_box_kld = avg_train_box_kld / len(point_train_batches)\n",
    "    avg_val_loss = avg_val_loss / len(point_val_batches)\n",
    "    avg_val_box_mse = avg_val_box_mse / len(point_val_batches)\n",
    "    avg_val_box_kld = avg_val_box_kld / len(point_val_batches)\n",
    "    \n",
    "\n",
    "    train_box = [torch.round(avg_train_box_mse, decimals=3),torch.round(avg_train_box_kld,decimals=3)]\n",
    "    val_box = [torch.round(avg_val_box_mse,decimals=3),torch.round(avg_val_box_kld,decimals=3)]\n",
    "\n",
    "    num_epoch.append(epoch)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    valid_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(textcolors.BLUE + \"End of epoch: {} | Stop Count: [{}/{}] | Time taken: {:.2f}\".format(epoch,count,wait,time.time() - start_time))\n",
    "    \n",
    "    print(textcolors.YELLOW + \"Box Resid: {}\".format(((y_boxes3d[0] - boxes[0])/y_boxes3d[0])*100))\n",
    "    print(\"Truth Box: {}\".format(y_boxes3d[0]))\n",
    "    print(\"Pred Box: {}\".format(boxes[0]))\n",
    "    print(textcolors.GREEN + \"(Average training) Total Loss: {:.3f} | Box MSE,KLD: {}\".format(avg_train_loss,train_box))\n",
    "    print(\"(Average validation) Total Loss: {:.3f} | Box MSE,KLD: {}\".format(avg_val_loss,val_box))\n",
    "\n",
    "    if min_validation_loss > avg_val_loss:\n",
    "        torch.save(model.state_dict(), 'lowest_val_loss.pt')\n",
    "        print(textcolors.YELLOW + \"New minimum validation loss--model saved!\")\n",
    "        min_validation_loss = avg_val_loss\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "        if count == wait:\n",
    "            print(textcolors.RED + \"Minimum validation loss has not changed for {} epochs. Exiting training now.\".format(wait))\n",
    "            break\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(num_epoch,train_losses,'-b',label='Training Loss')\n",
    "plt.plot(num_epoch,valid_losses,'-r',label='Validation Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Average Training and Validation Combined Loss, Batch Size: {}\".format(train_bs))\n",
    "plt.legend()\n",
    "plt.savefig('loss.png',bbox_inches='tight',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
